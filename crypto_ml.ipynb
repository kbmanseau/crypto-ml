{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto-ml.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0PXrEPnfzJxd"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: python-binance in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (0.7.4)\nRequirement already satisfied: cryptography in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2.8)\nRequirement already satisfied: requests in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2.22.0)\nRequirement already satisfied: pyOpenSSL in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.1.0)\nRequirement already satisfied: six in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (1.13.0)\nRequirement already satisfied: certifi in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2019.11.28)\nRequirement already satisfied: autobahn in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.11.1)\nRequirement already satisfied: chardet in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (3.0.4)\nRequirement already satisfied: dateparser in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (0.7.2)\nRequirement already satisfied: Twisted in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.10.0)\nRequirement already satisfied: urllib3 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (1.25.7)\nRequirement already satisfied: service-identity in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (18.1.0)\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from cryptography->python-binance) (1.13.2)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from requests->python-binance) (2.8)\nRequirement already satisfied: txaio>=18.8.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from autobahn->python-binance) (18.8.1)\nRequirement already satisfied: python-dateutil in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2.8.1)\nRequirement already satisfied: tzlocal in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2.0.0)\nRequirement already satisfied: regex in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2019.12.9)\nRequirement already satisfied: pytz in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2019.3)\nRequirement already satisfied: zope.interface>=4.4.2 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (4.7.1)\nRequirement already satisfied: constantly>=15.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (15.1.0)\nRequirement already satisfied: PyHamcrest>=1.9.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (1.9.0)\nRequirement already satisfied: incremental>=16.10.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (17.5.0)\nRequirement already satisfied: Automat>=0.3.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (0.8.0)\nRequirement already satisfied: attrs>=17.4.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (19.3.0)\nRequirement already satisfied: hyperlink>=17.1.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (19.0.0)\nRequirement already satisfied: pyasn1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from service-identity->python-binance) (0.4.8)\nRequirement already satisfied: pyasn1-modules in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from service-identity->python-binance) (0.2.7)\nRequirement already satisfied: pycparser in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->python-binance) (2.19)\nRequirement already satisfied: setuptools in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from zope.interface>=4.4.2->Twisted->python-binance) (41.2.0)\nWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
        }
      ],
      "source": [
        "!pip install python-binance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-haoiCcuL5u"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import math\n",
        "import argparse\n",
        "import json\n",
        "from binance.client import Client\n",
        "from datetime import datetime\n",
        "from dateutil import parser as par\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model Variables\n",
        "history_points = 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sunKqHVwvJiW"
      },
      "source": [
        "## Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oAW7l8whvl9z"
      },
      "source": [
        "### CSV To Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QWB4DXRdu7xo"
      },
      "outputs": [],
      "source": [
        "def csv_to_dataset(csv_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    #remove columns that are not needed\n",
        "    #timestamp, close_time, quote_av, trades, tb_base_av, tb_quote_av, ignore\n",
        "    data = data.drop('timestamp', axis=1)\n",
        "    data = data.drop('close_time', axis=1)\n",
        "    data = data.drop('quote_av', axis=1)\n",
        "    data = data.drop('trades', axis=1)\n",
        "    data = data.drop('tb_base_av', axis=1)\n",
        "    data = data.drop('tb_quote_av', axis=1)\n",
        "    data = data.drop('ignore', axis=1)\n",
        "\n",
        "    #Split the data into train and test sets before normalization\n",
        "    test_split = 0.9\n",
        "    n = int(len(data) * test_split)\n",
        "\n",
        "    d_train = data[:n]\n",
        "    d_test = data[n:]\n",
        "    #d_train, d_test = train_test_split(data, train_size=train_size, test_size=test_size, shuffle=False)\n",
        "\n",
        "    #Scale the data. Test data is scaled separately but with the same scaling parameters as the test data\n",
        "    data_normaliser = preprocessing.MinMaxScaler()\n",
        "    train_normalised = data_normaliser.fit_transform(d_train)\n",
        "    test_normalised = data_normaliser.transform(d_test)\n",
        "\n",
        "    #Get the data ready for model consumption\n",
        "    #using the last {history_points} open high low close volume data points, predict the next open value\n",
        "    #ohlcv_histories_normalised is a three dimensional array of size (len(data_normalised), history points, 5(open, high, low, close, volume))\n",
        "    #Each item in the list is an array of 50 days worth of ohlcv\n",
        "    ohlcv_train = np.array([train_normalised[i:i + history_points].copy() for i in range(len(train_normalised) - history_points)])\n",
        "    ohlcv_test = np.array([test_normalised[i:i + history_points].copy() for i in range(len(test_normalised) - history_points)])\n",
        "\n",
        "    #What is being predicted, will be compared for error\n",
        "    #ndov = next day open values\n",
        "    ndov_train_normalised = np.array([train_normalised[:, 0][i + history_points].copy() for i in range(len(train_normalised) - history_points)])\n",
        "    ndov_train_normalised = np.expand_dims(ndov_train_normalised, -1)\n",
        "\n",
        "    ndov_test_normalised = np.array([test_normalised[:, 0][i + history_points].copy() for i in range(len(test_normalised) - history_points)])\n",
        "    ndov_test_normalised = np.expand_dims(ndov_test_normalised, -1)\n",
        "\n",
        "    #Unormalised data for plotting later\n",
        "    ndov_train = np.array([d_train['open'][i + history_points].copy() for i in range(len(d_train) - history_points)])\n",
        "    ndov_train = np.expand_dims(ndov_train, -1)\n",
        "\n",
        "    ndov_test = np.array([d_test['open'][i + len(d_train) + history_points].copy() for i in range(len(d_test) - history_points)])\n",
        "    ndov_test = np.expand_dims(ndov_test, -1)\n",
        "\n",
        "    #Testing with delta price\n",
        "\n",
        "\n",
        "    #Variables to scale the data back up later\n",
        "    y_normaliser = preprocessing.MinMaxScaler()\n",
        "    y_train = y_normaliser.fit_transform(ndov_train)\n",
        "    y_test = y_normaliser.transform(ndov_test)\n",
        "\n",
        "    #tis - technical indicators\n",
        "    tis_train = []\n",
        "    tis_test = []\n",
        "    for his in ohlcv_train:\n",
        "        # note since we are using his[3] we are taking the SMA of the closing price\n",
        "        sma = np.mean(his[:, 3])\n",
        "        tis_train.append(np.array([sma]))\n",
        "    for his in ohlcv_test:\n",
        "        sma = np.mean(his[:, 3])\n",
        "        tis_test.append(np.array([sma]))\n",
        "\n",
        "    tis_train = np.array(tis_train)\n",
        "    tis_test = np.array(tis_test)\n",
        "\n",
        "    indicator_normaliser = preprocessing.MinMaxScaler()\n",
        "    tis_normalised_train = indicator_normaliser.fit_transform(tis_train)\n",
        "    tis_normalised_test = indicator_normaliser.transform(tis_test)\n",
        "\n",
        "    #assert ohlcv_histories_normalised.shape[0] == next_day_open_values_normalised.shape[0] == technical_indicators_normalised.shape[0]\n",
        "    #return ohlcv_histories_normalised, technical_indicators_normalised, next_day_open_values_normalised, next_day_open_values, y_normaliser\n",
        "\n",
        "    return ohlcv_train, \\\n",
        "        ohlcv_test, \\\n",
        "        ndov_test, \\\n",
        "        y_train, \\\n",
        "        y_test, \\\n",
        "        y_normaliser, \\\n",
        "        tis_normalised_train, \\\n",
        "        tis_normalised_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PVyVvy5HvqFb"
      },
      "source": [
        "### Save Data To CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aPXTNH6HwK2M"
      },
      "outputs": [],
      "source": [
        "def get_all_binance(symbol, kline_size, data_type, save = True):\n",
        "    #https://medium.com/swlh/retrieving-full-historical-data-for-every-cryptocurrency-on-binance-bitmex-using-the-python-apis-27b47fd8137f\n",
        "\n",
        "    def time_differencing(td_data_df):\n",
        "      for i in td_data_df.index:\n",
        "        if i < len(td_data_df)-1:\n",
        "            td_data_df.loc[i,\"td\"] = float(td_data_df.loc[i+1,\"open\"]) - float(td_data_df.loc[i, \"open\"])\n",
        "        else:\n",
        "            td_data_df.loc[i,'td'] = 0\n",
        "      return td_data_df\n",
        "\n",
        "    #Only download new data instead of downloading the entire set every runtime\n",
        "    def minutes_of_new_data(symbol, kline_size, data, client, source):\n",
        "        if len(data) > 0:\n",
        "            old = par.parse(data[\"timestamp\"].iloc[-1])\n",
        "        elif source == \"binance\":\n",
        "            old = datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
        "        if source == \"binance\":\n",
        "            new = pd.to_datetime(binance_client.get_klines(symbol=symbol, interval=kline_size)[-1][0], unit='ms')\n",
        "        return old, new \n",
        "    \n",
        "    ### API\n",
        "    binance_api_key = \"JHiOfaAiYmC2uQVeaPPlNZH1SGZCazrRKX1wDU0ahBIq5omUPoWbE73yWXx663PW\"\n",
        "    binance_api_secret = \"R82RLAUqbsvBmuy9e58U5QU8GFWDgvDVJx2oc6bLx4n4MQ5qEguFYHeoNY5uoMyz\"\n",
        "    binance_client = Client(api_key=binance_api_key, api_secret=binance_api_secret)\n",
        "\n",
        "    #Variables\n",
        "    binsizes = {\"1m\": 1, \"5m\": 5, \"1h\": 60, \"1d\": 1440}\n",
        "    if data_type!=\"td\":\n",
        "        filename = '%s-%s-data.csv' % (symbol, kline_size)\n",
        "    else:\n",
        "        filename = '%s-%s-data-td.csv' % (symbol, kline_size)\n",
        "\n",
        "    #If file exists, read it, otherwise create new object\n",
        "    if os.path.isfile(filename):\n",
        "        data_df = pd.read_csv(filename)\n",
        "    else:\n",
        "        data_df = pd.DataFrame()\n",
        "    \n",
        "    #Perform checks for only downloading new data\n",
        "    oldest_point, newest_point = minutes_of_new_data(symbol, kline_size, data_df, binance_client, source = \"binance\")\n",
        "    if data_type == \"reg\":\n",
        "        delta_min = (newest_point - oldest_point).total_seconds()/60\n",
        "        available_data = math.ceil(delta_min/binsizes[kline_size])\n",
        "        if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'):\n",
        "            print('Downloading all available %s data for %s. Be patient..!' % (kline_size, symbol))\n",
        "        else:\n",
        "            print('Downloading %d minutes of new data available for %s, i.e. %d instances of %s data.' % (delta_min, symbol, available_data, kline_size))\n",
        "    elif data_type == \"td\":\n",
        "        delta_day = (newest_point - oldest_point).total_seconds()/86400\n",
        "        available_data = math.ceil(delta_day/binsizes[kline_size])\n",
        "        if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'):\n",
        "            print('Downloading all available %s data for %s. Be patient..!' % (kline_size, symbol))\n",
        "        else:\n",
        "            print('Downloading %d days of new data available for %s, i.e. %d instances of %s data.' % (delta_day, symbol, available_data, kline_size))\n",
        "    \n",
        "    klines = binance_client.get_historical_klines(symbol, kline_size, oldest_point.strftime(\"%d %b %Y %H:%M:%S\"), newest_point.strftime(\"%d %b %Y %H:%M:%S\"))\n",
        "    data = pd.DataFrame(klines, columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore' ])\n",
        "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
        "\n",
        "    if len(data_df) > 0:\n",
        "        if(oldest_point != newest_point):\n",
        "            temp_df = pd.DataFrame(data)\n",
        "            data_df = data_df.append(temp_df, sort=False)\n",
        "    else:\n",
        "        data_df = data\n",
        "\n",
        "    if data_type == \"td\":\n",
        "        #Create column allowing for referincing index by an int\n",
        "        x = 0\n",
        "        for i in data_df.index:\n",
        "            data_df.loc[i, 'counter'] = x\n",
        "            x = x+1\n",
        "\n",
        "        data_df.set_index('counter', inplace=True)\n",
        "    else:\n",
        "        data_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    if save:\n",
        "        if data_type == \"td\": #Saved format will be time differenced\n",
        "            data_df = time_differencing(data_df)\n",
        "        data_df.to_csv(filename)\n",
        "        \n",
        "    print('All caught up..!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "51tL4s-6vRHA"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gz5xzbJ9vcgx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jDGPPqfxJnZ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRfv5FdfxP39"
      },
      "source": [
        "### Download latest data to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "yTfTr1eexM_M",
        "outputId": "29f9d729-89a0-4ff7-f787-dcd02129d7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading all available 1d data for ETHUSDT. Be patient..!\n",
            "All caught up..!\n"
          ]
        }
      ],
      "source": [
        "##Environment variables\n",
        "symbol = \"ETHUSDT\"      #Crypto pair on binance\n",
        "kline_size = \"1d\"       #1h or 1d\n",
        "data_type = \"td\"        #reg or td\n",
        "save = True             #Save data to csv file\n",
        "\n",
        "get_all_binance(symbol, kline_size, data_type, save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RFtIjDcJxV8I"
      },
      "source": [
        "### Train Model"
      ]
    }
  ]
}