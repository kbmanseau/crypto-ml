{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto-ml.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0PXrEPnfzJxd"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: python-binance in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (0.7.4)\nRequirement already satisfied: urllib3 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (1.25.7)\nRequirement already satisfied: six in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (1.13.0)\nRequirement already satisfied: chardet in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (3.0.4)\nRequirement already satisfied: cryptography in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2.8)\nRequirement already satisfied: Twisted in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.10.0)\nRequirement already satisfied: autobahn in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.11.1)\nRequirement already satisfied: requests in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2.22.0)\nRequirement already satisfied: dateparser in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (0.7.2)\nRequirement already satisfied: pyOpenSSL in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (19.1.0)\nRequirement already satisfied: service-identity in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (18.1.0)\nRequirement already satisfied: certifi in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from python-binance) (2019.11.28)\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from cryptography->python-binance) (1.13.2)\nRequirement already satisfied: constantly>=15.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (15.1.0)\nRequirement already satisfied: incremental>=16.10.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (17.5.0)\nRequirement already satisfied: Automat>=0.3.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (0.8.0)\nRequirement already satisfied: zope.interface>=4.4.2 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (4.7.1)\nRequirement already satisfied: PyHamcrest>=1.9.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (1.9.0)\nRequirement already satisfied: attrs>=17.4.0 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (19.3.0)\nRequirement already satisfied: hyperlink>=17.1.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from Twisted->python-binance) (19.0.0)\nRequirement already satisfied: txaio>=18.8.1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from autobahn->python-binance) (18.8.1)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from requests->python-binance) (2.8)\nRequirement already satisfied: regex in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2019.12.9)\nRequirement already satisfied: python-dateutil in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2.8.1)\nRequirement already satisfied: pytz in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2019.3)\nRequirement already satisfied: tzlocal in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from dateparser->python-binance) (2.0.0)\nRequirement already satisfied: pyasn1-modules in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from service-identity->python-binance) (0.2.7)\nRequirement already satisfied: pyasn1 in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from service-identity->python-binance) (0.4.8)\nRequirement already satisfied: pycparser in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->python-binance) (2.19)\nRequirement already satisfied: setuptools in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (from zope.interface>=4.4.2->Twisted->python-binance) (41.2.0)\nRequirement already satisfied: joblib in c:\\users\\kyler\\documents\\visual studio code\\crypto-ml\\.venv\\lib\\site-packages (0.14.1)\n"
        }
      ],
      "source": [
        "!pip install python-binance\n",
        "!pip install joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-haoiCcuL5u"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import os.path\n",
        "import math\n",
        "import argparse\n",
        "import json\n",
        "import joblib\n",
        "from binance.client import Client\n",
        "from datetime import datetime\n",
        "from dateutil import parser as par\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model Variables\n",
        "history_points = 14\n",
        "csv_path = \"ETHUSDT-1d-data-td.csv\"\n",
        "scaler_x_filename = \"x_scaler.save\"\n",
        "scaler_td_filename = \"td_scaler.save\"\n",
        "scaler_ti_filename = \"ti_scaler.save\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sunKqHVwvJiW"
      },
      "source": [
        "## Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oAW7l8whvl9z"
      },
      "source": [
        "### CSV To Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def csv_to_dataset(csv_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    #remove columns that are not needed\n",
        "    #timestamp, close_time, quote_av, trades, tb_base_av, tb_quote_av, ignore\n",
        "    data = data.drop('counter', axis=1)\n",
        "    data = data.drop('timestamp', axis=1)\n",
        "    data = data.drop('close_time', axis=1)\n",
        "    data = data.drop('quote_av', axis=1)\n",
        "    data = data.drop('trades', axis=1)\n",
        "    data = data.drop('tb_base_av', axis=1)\n",
        "    data = data.drop('tb_quote_av', axis=1)\n",
        "    data = data.drop('ignore', axis=1)\n",
        "\n",
        "    td_data = data\n",
        "    data = data.drop('td', axis=1)\n",
        "\n",
        "    #Split the data into train and test sets before normalization\n",
        "    test_split = 0.9\n",
        "    n = int(len(data) * test_split)\n",
        "\n",
        "    d_train = data[:n]\n",
        "    td_train = td_data[:n]\n",
        "    d_test = data[n:]\n",
        "    td_test = td_data[n:]\n",
        "\n",
        "    #Scale the data. Test data is scaled separately but with the same scaling parameters as the test data\n",
        "    data_normaliser = preprocessing.MinMaxScaler()\n",
        "    train_normalised = data_normaliser.fit_transform(d_train)\n",
        "    test_normalised = data_normaliser.transform(d_test)\n",
        "\n",
        "    #Save the scaler to scale new data for predictions\n",
        "    joblib.dump(data_normaliser, scaler_x_filename)\n",
        "\n",
        "    #Get the data ready for model consumption\n",
        "    #using the last {history_points} open high low close volume data points, predict the next open value\n",
        "    #ohlcv_histories_normalised is a three dimensional array of size (len(data_normalised), history points, 5(open, high, low, close, volume))\n",
        "    #Each item in the list is an array of 50 days worth of ohlcv\n",
        "    ohlcv_train = np.array([train_normalised[i:i + history_points].copy() for i in range(len(train_normalised) - history_points)])\n",
        "    ohlcv_test = np.array([test_normalised[i:i + history_points].copy() for i in range(len(test_normalised) - history_points)])\n",
        "\n",
        "    #Values model is trying to predict\n",
        "    y_train = np.array(td_train['td'])\n",
        "    y_train = np.expand_dims(y_train, -1)\n",
        "    #Strip of initial history_points amount of data points since the model needs atleast history_points history\n",
        "    y_train = y_train[history_points:]\n",
        "\n",
        "\n",
        "    y_test = np.array(td_test['td'])\n",
        "    y_test = np.expand_dims(y_test, -1)\n",
        "    #Strip of initial history_points amount of data points since the model needs atleast history_points history\n",
        "    y_test = y_test[history_points:]\n",
        "\n",
        "    #Scale the data. Test data is scaled separately but with the same scaling parameters as the test data\n",
        "    td_normaliser = preprocessing.MinMaxScaler()\n",
        "    y_train = td_normaliser.fit_transform(y_train)\n",
        "    y_test = td_normaliser.transform(y_test)\n",
        "\n",
        "    #Save the scaler to scale new data for predictions\n",
        "    joblib.dump(td_normaliser, scaler_td_filename)\n",
        "\n",
        "    #tis - technical indicators\n",
        "    tis_train = []\n",
        "    tis_test = []\n",
        "    for his in ohlcv_train:\n",
        "        # note since we are using his[3] we are taking the SMA of the closing price\n",
        "        sma = np.mean(his[:, 3])\n",
        "        tis_train.append(np.array([sma]))\n",
        "    for his in ohlcv_test:\n",
        "        sma = np.mean(his[:, 3])\n",
        "        tis_test.append(np.array([sma]))\n",
        "\n",
        "    tis_train = np.array(tis_train)\n",
        "    tis_test = np.array(tis_test)\n",
        "\n",
        "    indicator_normaliser = preprocessing.MinMaxScaler()\n",
        "    tis_normalised_train = indicator_normaliser.fit_transform(tis_train)\n",
        "    tis_normalised_test = indicator_normaliser.transform(tis_test)\n",
        "\n",
        "    #Save the scaler of the technical indicators\n",
        "    joblib.dump(indicator_normaliser, scaler_ti_filename)\n",
        "\n",
        "    #assert ohlcv_histories_normalised.shape[0] == next_day_open_values_normalised.shape[0] == technical_indicators_normalised.shape[0]\n",
        "    #return ohlcv_histories_normalised, technical_indicators_normalised, next_day_open_values_normalised, next_day_open_values, y_normaliser\n",
        "\n",
        "    return ohlcv_train, \\\n",
        "        ohlcv_test, \\\n",
        "        y_train, \\\n",
        "        y_test, \\\n",
        "        tis_normalised_train, \\\n",
        "        tis_normalised_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PVyVvy5HvqFb"
      },
      "source": [
        "### Save Data To CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_binance(symbol, kline_size, data_type, save = True):\n",
        "    #https://medium.com/swlh/retrieving-full-historical-data-for-every-cryptocurrency-on-binance-bitmex-using-the-python-apis-27b47fd8137f\n",
        "\n",
        "    def time_differencing(td_data_df):\n",
        "      for i in td_data_df.index:\n",
        "        if i < len(td_data_df)-1:\n",
        "            td_data_df.loc[i,\"td\"] = float(td_data_df.loc[i+1,\"open\"]) - float(td_data_df.loc[i, \"open\"])\n",
        "        else:\n",
        "            td_data_df.loc[i,'td'] = 0\n",
        "      return td_data_df\n",
        "\n",
        "    #Only download new data instead of downloading the entire set every runtime\n",
        "    def minutes_of_new_data(symbol, kline_size, data, client, source):\n",
        "        if len(data) > 0:\n",
        "            old = par.parse(data[\"timestamp\"].iloc[-1])\n",
        "        elif source == \"binance\":\n",
        "            old = datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
        "        if source == \"binance\":\n",
        "            new = pd.to_datetime(binance_client.get_klines(symbol=symbol, interval=kline_size)[-1][0], unit='ms')\n",
        "        return old, new \n",
        "    \n",
        "    ### API\n",
        "    binance_api_key = \"JHiOfaAiYmC2uQVeaPPlNZH1SGZCazrRKX1wDU0ahBIq5omUPoWbE73yWXx663PW\"\n",
        "    binance_api_secret = \"R82RLAUqbsvBmuy9e58U5QU8GFWDgvDVJx2oc6bLx4n4MQ5qEguFYHeoNY5uoMyz\"\n",
        "    binance_client = Client(api_key=binance_api_key, api_secret=binance_api_secret)\n",
        "\n",
        "    #Variables\n",
        "    binsizes = {\"1m\": 1, \"5m\": 5, \"1h\": 60, \"1d\": 1440}\n",
        "    if data_type!=\"td\":\n",
        "        filename = '%s-%s-data.csv' % (symbol, kline_size)\n",
        "    else:\n",
        "        filename = '%s-%s-data-td.csv' % (symbol, kline_size)\n",
        "\n",
        "    #If file exists, read it, otherwise create new object\n",
        "    if os.path.isfile(filename):\n",
        "        data_df = pd.read_csv(filename)\n",
        "    else:\n",
        "        data_df = pd.DataFrame()\n",
        "    \n",
        "    #Perform checks for only downloading new data\n",
        "    oldest_point, newest_point = minutes_of_new_data(symbol, kline_size, data_df, binance_client, source = \"binance\")\n",
        "    if data_type == \"reg\":\n",
        "        delta_min = (newest_point - oldest_point).total_seconds()/60\n",
        "        available_data = math.ceil(delta_min/binsizes[kline_size])\n",
        "        if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'):\n",
        "            print('Downloading all available %s data for %s. Be patient..!' % (kline_size, symbol))\n",
        "        else:\n",
        "            print('Downloading %d minutes of new data available for %s, i.e. %d instances of %s data.' % (delta_min, symbol, available_data, kline_size))\n",
        "    elif data_type == \"td\":\n",
        "        delta_day = (newest_point - oldest_point).total_seconds()/86400\n",
        "        available_data = math.ceil(delta_day/binsizes[kline_size])\n",
        "        if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'):\n",
        "            print('Downloading all available %s data for %s. Be patient..!' % (kline_size, symbol))\n",
        "        else:\n",
        "            print('Downloading %d days of new data available for %s, i.e. %d instances of %s data.' % (delta_day, symbol, available_data, kline_size))\n",
        "    \n",
        "    klines = binance_client.get_historical_klines(symbol, kline_size, oldest_point.strftime(\"%d %b %Y %H:%M:%S\"), newest_point.strftime(\"%d %b %Y %H:%M:%S\"))\n",
        "    data = pd.DataFrame(klines, columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore' ])\n",
        "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
        "\n",
        "    if len(data_df) > 0:\n",
        "        if(oldest_point != newest_point):\n",
        "            temp_df = pd.DataFrame(data)\n",
        "            data_df = data_df.append(temp_df, sort=False)\n",
        "    else:\n",
        "        data_df = data\n",
        "\n",
        "    if data_type == \"td\":\n",
        "        #Create column allowing for referincing index by an int\n",
        "        x = 0\n",
        "        for i in data_df.index:\n",
        "            data_df.loc[i, 'counter'] = x\n",
        "            x = x+1\n",
        "\n",
        "        data_df.set_index('counter', inplace=True)\n",
        "    else:\n",
        "        data_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    if save:\n",
        "        if data_type == \"td\": #Saved format will be time differenced\n",
        "            data_df = time_differencing(data_df)\n",
        "        data_df.to_csv(filename)\n",
        "        \n",
        "    print('All caught up..!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jDGPPqfxJnZ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRfv5FdfxP39"
      },
      "source": [
        "### Download latest data to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Downloading 0 days of new data available for ETHUSDT, i.e. 0 instances of 1d data.\nAll caught up..!\n"
        }
      ],
      "source": [
        "##Environment variables\n",
        "symbol = \"ETHUSDT\"      #Crypto pair on binance\n",
        "kline_size = \"1d\"       #1h or 1d\n",
        "data_type = \"td\"        #reg or td\n",
        "save = True             #Save data to csv file\n",
        "\n",
        "get_all_binance(symbol, kline_size, data_type, save)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Extract data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "ohlcv_histories_train, ohlcv_histories_test, y_train, y_test, tech_ind_train, tech_ind_test = csv_to_dataset(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define two sets of inputs\n",
        "lstm_input = tf.keras.layers.Input(shape=(history_points, 5), name='lstm_input')\n",
        "dense_input = tf.keras.layers.Input(shape=(tech_ind_train.shape[1],), name='tech_input')\n",
        "\n",
        "# the first branch operates on the first input\n",
        "x = tf.keras.layers.LSTM(history_points, name='lstm_0')(lstm_input)\n",
        "x = tf.keras.layers.Dropout(0.2, name='lstm_dropout_0')(x)\n",
        "lstm_branch = tf.keras.models.Model(inputs=lstm_input, outputs=x)\n",
        "\n",
        "# the second branch opreates on the second input\n",
        "y = tf.keras.layers.Dense(20, name='tech_dense_0')(dense_input)\n",
        "y = tf.keras.layers.Activation(\"relu\", name='tech_relu_0')(y)\n",
        "y = tf.keras.layers.Dropout(0.2, name='tech_dropout_0')(y)\n",
        "technical_indicators_branch = tf.keras.models.Model(inputs=dense_input, outputs=y)\n",
        "\n",
        "# combine the output of the two branches\n",
        "combined = tf.keras.layers.concatenate([lstm_branch.output, technical_indicators_branch.output], name='concatenate')\n",
        "\n",
        "z = tf.keras.layers.Dense(64, activation=\"sigmoid\", name='dense_pooling')(combined)\n",
        "z = tf.keras.layers.Dense(1, activation=\"linear\", name='dense_out')(z)\n",
        "\n",
        "# our model will accept the inputs of the two branches and\n",
        "# then output a single value\n",
        "model = tf.keras.models.Model(inputs=[lstm_branch.input, technical_indicators_branch.input], outputs=z)\n",
        "adam = tf.keras.optimizers.Adam(lr=0.0005)\n",
        "model.compile(optimizer=adam, loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RFtIjDcJxV8I"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-29-8382ef4ff000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"logs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"fit\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[0mtensorboard_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
          ]
        }
      ],
      "source": [
        "bs = 1024\n",
        "e = 100\n",
        "restore_model = True\n",
        "\n",
        "#logging for tensorboard\n",
        "log_dir= os.path.join(\n",
        "    \"logs\",\n",
        "    \"fit\",\n",
        "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "if(restore_model):\n",
        "    model = tf.keras.models.load_model('technical_model.h5')\n",
        "\n",
        "else:\n",
        "    #print(ohlcv_histories_train)\n",
        "    model.fit(x=[ohlcv_histories_train, tech_ind_train],\n",
        "          y=y_train,\n",
        "          batch_size=bs,\n",
        "          epochs=e,\n",
        "          shuffle=True,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[tensorboard_callback]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load in the scalers\n",
        "data_scaler = joblib.load(scaler_x_filename)\n",
        "tis_scaler = joblib.load(scaler_ti_filename)\n",
        "td_scaler = joblib.load(scaler_td_filename)\n",
        "\n",
        "# evaluation\n",
        "\n",
        "y_test_predicted = model.predict([ohlcv_histories_test, tech_ind_test])\n",
        "y_test_predicted = td_scaler.inverse_transform(y_test_predicted)\n",
        "y_predicted = model.predict([ohlcv_histories_train, tech_ind_train])\n",
        "y_predicted = td_scaler.inverse_transform(y_predicted)\n",
        "y_train = td_scaler.inverse_transform(y_train)\n",
        "y_test = td_scaler.inverse_transform(y_test)\n",
        "assert y_test.shape == y_test_predicted.shape\n",
        "\n",
        "real_mse = np.mean(np.square(y_test - y_test_predicted))\n",
        "scaled_mse = real_mse / (np.max(y_test) - np.min(y_test)) * 100\n",
        "print(scaled_mse)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.gcf().set_size_inches(22, 15, forward=True)\n",
        "\n",
        "start = 0\n",
        "end = -1\n",
        "\n",
        "real = plt.plot(y_test[start:end], label='real')\n",
        "pred = plt.plot(y_test_predicted[start:end], label='predicted')\n",
        "\n",
        "#real = plt.plot(y_train[start:end], label='real_train')\n",
        "#pred = plt.plot(y_predicted[start:end], label='predicted_train')\n",
        "plt.legend(['Real', 'Predicted'])\n",
        "\n",
        "plt.savefig(\"test_data.png\")\n",
        "plt.show()\n",
        "\n",
        "from datetime import datetime\n",
        "model.save(f'technical_model.h5')"
      ]
    }
  ]
}